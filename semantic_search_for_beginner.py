# -*- coding: utf-8 -*-
"""semantic-search-for-beginner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Neeshamraghav012/Data-Weapons/blob/master/semantic-search-for-beginner.ipynb

<a href="https://www.kaggle.com/code/neesham/semantic-search-for-beginners?scriptVersionId=125121064" target="_blank"><img align="left" alt="Kaggle" title="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"></a>

# But what is semantic search?

Semantic search is a technique for searching text data using meaning or context to match the search query with relevant results. Unlike traditional search algorithms that rely solely on keywords, semantic search uses Natural Language Processing (NLP) and Machine Learning (ML) to understand the intent behind the search query and the context of the text data. This results in more accurate and relevant search results compared to keyword-based search.

In this notebook, we will explore the basics of semantic. If you want to experiment with semantic search hit the copy and edit button.

*Upvote the notebook if you found it usefull â¤ï¸!*

# Importing Packages
"""

import numpy as np
import pandas as pd

"""# Installing datasets, evalute, transformers and faiss (Facebook AI Similarity Search)."""

"""## Loading IMDB Movies Dataset"""

df = pd.read_excel("Dronealexa.xlsx")


data = df.drop_duplicates(subset=['Abstract'], keep='first')
data = data.reset_index(drop=True)






"""We only need Series_Title, Genre, Overview and Director for search purpose. Because in most of the cases we search movies by these features."""

data = data[['Title', 'Abstract']]

"""## Converting pandas dataframe to Huggingface dataset.
Because it is easy to use and we can use Huggingface tokenizers and models directly on huggingface dataset objects.
"""

from datasets import Dataset

dfm = Dataset.from_pandas(data)


"""Concatenating all the text field so that we can make a single embedding vector for all the relevant data.

"""

def concatenate_text(data):

    return {"text": data['Title'] + '\n' + data['Abstract']}


dfm = dfm.map(concatenate_text)



dfm.to_csv("concatenated_dataset.csv", index=False)

"""### Result of concatenation"""




"""## Importing Model and Tokenizer from HuggingFace"""

from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)

"""> We need a single vector for our data so we need to average our token embeddings.One popular approach is to perform CLS pooling on our modelâ€™s outputs, where we simply collect the last hidden state for the special [CLS] token. The following function does the trick for us:"""

def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]

def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)

"""## Debugging the Output"""

#embedding = get_embeddings(movie_dataset['text'][0])

#embedding

"""# Now let's apply the function to the whole dataset.

### This will take some time so be patient ðŸ™ƒ.
"""

embeddings_dataset = pd.read_csv("embeddings_dataset.csv")

# Debugging


# Uncomment to debug the output

embeddings_dataset['embeddings'][0]

"""# Using FAISS for efficient similarity search"""

embeddings_dataset.add_faiss_index(column="embeddings")

import faiss
# Save the Faiss index separately
faiss.write_index(embeddings_dataset.faiss_index, "embeddings_dataset.faiss")

"""# Testing"""

question = "what are aerial drones?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape

scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)

samples_df

"""# Results"""

for _, row in samples_df.iterrows():
    print(f"Series Title: {row.Title}")
    print(f"Overview: {row.Abstract}")

    print("=" * 50)
    print()

"""# Conclusion:

In this notebook we experimented with semantic search. We get to know about what is semantic search and how it can be used to search efficiently. Do you know YouTube uses semantic search? Due to this you get better search results.

## Activity:

Try to search a song on Spotify be its lyrics, after that search the same song on YouTube. Did you find any difference? Let me know in the comment section!

### Thanks for reading this notebook. Upvote it if you found it useful ðŸ˜‡.
### Checkout my other notebooks ðŸ™ƒ
* [XGBoost V/S LightGBM](https://www.kaggle.com/code/neesham/xgboost-v-s-lightgbm)
* [ðŸ”¥ Pandas V/S SQL](https://www.kaggle.com/code/neesham/pandas-v-s-sql)
* [ðŸ”¥ Transformers for Beginners (P1)](https://www.kaggle.com/code/neesham/transformers-for-beginners-p1)
"""